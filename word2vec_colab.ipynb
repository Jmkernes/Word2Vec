{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from skipgrams import SkipGrams\n",
    "from data_utils import DataManager\n",
    "from utils import *\n",
    "from model import Word2Vec, loss_fn\n",
    "from functools import partial\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size: 11. Threshold: 1.00e-02\n",
      "Getting dataset size...\n",
      "There are 17556 total lines in the dataset.\n",
      "Sorting words by frequency...% completed.\n",
      "Creating keys and unigrams...\n",
      "Initializing lookup tables...\n",
      "Finished.\n",
      "Finding number of tokens...\n",
      "Total number of tokens: 2462984\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 11\n",
    "threshold = 1e-3\n",
    "dm = DataManager.from_text_file('train.txt', WINDOW_SIZE, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an analogies metric to print every so often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_common_analogies(model, dm):\n",
    "    \n",
    "    U = model.layers[0].get_weights()[0]\n",
    "    V = model.layers[1].get_weights()[0]\n",
    "    W_emb = 0.5*(U+V)\n",
    "    \n",
    "    func = partial(print_analogies, U=W_emb, vocab_table=dm.vocab_table,\n",
    "                  inv_vocab_table=dm.inv_vocab_table, K=3)\n",
    "    \n",
    "    print('~'*20+' Analogy task '+'~'*20)\n",
    "    func('he', 'she', 'him')\n",
    "    print('-'*40)\n",
    "    func('see', 'saw', 'hear')\n",
    "    print('-'*40)\n",
    "    func('m', 'km', 'ft')\n",
    "    print('-'*40)\n",
    "    func('possible', 'impossible', 'able')\n",
    "    print('-'*40)\n",
    "    func('day', 'week', 'month')\n",
    "    print('-'*40)\n",
    "    func('daughter', 'son', 'mother')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "VOCAB_SIZE = dm.skg.vocab_size+1\n",
    "history = {'loss':[]}\n",
    "\n",
    "# model\n",
    "tf.keras.backend.clear_session()\n",
    "model = Word2Vec(vocab_size=VOCAB_SIZE, d_model=128)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, ctxt, lbl, mask):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model((inp, ctxt))\n",
    "        loss = loss_fn(lbl, pred, mask)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NS = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1\n",
    "DS_SIZE = dm.num_tokens//BATCH_SIZE\n",
    "BUFFER_SIZE = 5000 # required buffer memory can become large ~ roughly bsz*(num_ns+window_size-1)*buffer*32*3 bytes.\n",
    "\n",
    "train_ds = dm.batched_ds(BATCH_SIZE, NUM_NS, BUFFER_SIZE).prefetch(1)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"----- Epoch {epoch+1}/{EPOCHS} -----\")\n",
    "    train_loss.reset_states()\n",
    "    start = time.time()\n",
    "    for step, ((inp, ctxt), lbl, mask) in enumerate(train_ds):\n",
    "        \n",
    "        train_step(inp, ctxt, lbl, mask)\n",
    "        loss = train_loss.result().numpy()\n",
    "        diff = (time.time()-start)/(step+1)\n",
    "        history['loss'].append(loss)\n",
    "        print_bar(step, DS_SIZE, diff, loss)\n",
    "        \n",
    "        # we start the drop threshold off high to get some training for \n",
    "        # common words, then decrease it over training to learn rare words\n",
    "        if (step+1)%500==0 and threshold > 1e-5:\n",
    "            threshold /= 10\n",
    "            dm.skg.set_threshold(threshold)\n",
    "            \n",
    "        if (step+1)%1000==0:\n",
    "            test_common_analogies(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract embedding as average of input+output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'zhou', b'various', b'nations', b'less', b'wheeler', b'national',\n",
       "       b'european', b'sold', b'royal', b'works'], dtype=object)"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = model.layers[0].get_weights()[0]\n",
    "V = model.layers[1].get_weights()[0]\n",
    "W_emb = 0.5*(U+V)\n",
    "find_closest('fun', W_emb, dm.vocab_table, dm.inv_vocab_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "print_analogy = partial(print_analogies, U=W_emb, \n",
    "    vocab_table=dm.vocab_table, inv_vocab_table=dm.inv_vocab_table, K=3)\n",
    "\n",
    "print_close = partial(print_closest, U=W_emb, vocab_table=dm.vocab_table,\n",
    "    inv_vocab_table=dm.inv_vocab_table, K=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Have fun) testing some analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~ Analogy task ~~~~~~~~~~~~~~~~~~~~\n",
      "he is to she, as him is to ___?\n",
      "\tOption 1: b'films'\n",
      "\tOption 2: b'wheeler'\n",
      "\tOption 3: b'department'\n",
      "----------------------------------------\n",
      "some is to many, as few is to ___?\n",
      "\tOption 1: b'weeks'\n",
      "\tOption 2: b'moved'\n",
      "\tOption 3: b'although'\n",
      "----------------------------------------\n",
      "m is to km, as ft is to ___?\n",
      "\tOption 1: b'h'\n",
      "\tOption 2: b'@,@'\n",
      "\tOption 3: b'/'\n",
      "----------------------------------------\n",
      "person is to people, as one is to ___?\n",
      "\tOption 1: b\"'s\"\n",
      "\tOption 2: b'that'\n",
      "\tOption 3: b'by'\n",
      "----------------------------------------\n",
      "day is to week, as month is to ___?\n",
      "\tOption 1: b'helped'\n",
      "\tOption 2: b'space'\n",
      "\tOption 3: b'arts'\n",
      "----------------------------------------\n",
      "daughter is to son, as mother is to ___?\n",
      "\tOption 1: b'back'\n",
      "\tOption 2: b'states'\n",
      "\tOption 3: b'its'\n"
     ]
    }
   ],
   "source": [
    "print('~'*20+' Analogy task '+'~'*20)\n",
    "print_analogy('he', 'she', 'him')\n",
    "print('-'*40)\n",
    "print_analogy('some', 'many', 'few')\n",
    "print('-'*40)\n",
    "print_analogy('m', 'km', 'ft')\n",
    "print('-'*40)\n",
    "print_analogy('person', 'people', 'one')\n",
    "print('-'*40)\n",
    "print_analogy('day', 'week', 'month')\n",
    "print('-'*40)\n",
    "print_analogy('daughter', 'son', 'mother')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~ Word grouping task ~~~~~~~~~~~~~~~~~~~~\n",
      "The nearest neighbors to president are:\n",
      "\tNeighbor 1: b'national'\n",
      "\tNeighbor 2: b'singles'\n",
      "\tNeighbor 3: b'army'\n",
      "----------------------------------------\n",
      "The nearest neighbors to game are:\n",
      "\tNeighbor 1: b'their'\n",
      "\tNeighbor 2: b'which'\n",
      "\tNeighbor 3: b'@-@'\n",
      "----------------------------------------\n",
      "The nearest neighbors to company are:\n",
      "\tNeighbor 1: b'which'\n",
      "\tNeighbor 2: b'but'\n",
      "\tNeighbor 3: b'their'\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('~'*20+' Word grouping task '+'~'*20)\n",
    "for word in ['president', 'game', 'company']:\n",
    "    print_close(word)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28913"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.skg.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28913"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dm.skg.unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([28914])>"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.tokenize(tf.constant('422523'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m          tests.py             valid.txt\r\n",
      "data_utils.py        train.py             word2vec_colab.ipynb\r\n",
      "model.py             train.txt\r\n",
      "skipgrams.py         utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [0.]+list(dm.skg.unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.strings.split(dm.detokenize(tf.range(dm.skg.vocab_size+1))).numpy()\n",
    "words = [x.decode() for x in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(zip(words, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<sos>', '<eos>', '<unk>', 'the']"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.decode() for x in words[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"loss\": [4.5, 3.2, 2.1]}'"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = {'loss':[4.5, 3.2, 2.1]}\n",
    "json.dumps(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
